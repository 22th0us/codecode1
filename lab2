import tensorflow as tf  # tensorflow를 tf로 import
import numpy as np   # numpy를 np로 import

tf.enable_eager_execution()   # 그래프 기반 실행 모드에서 실행 모드로 변경하여 사용 가능

# x_data, y_data 값 지정, x_data는 input y_data는 출력, 입력을 그대로 출력해주는 모델을 만들어 줄 것
# H(x) = Wx + b 에서 w, b값을 예측 -> x, y값이 같기 위해서 x는 1, y는 0외 되어야 함
x_data = [1, 2, 3, 4, 5]
y_data = [1, 2, 3, 4, 5]

# Variable을 이용해 W, b 값의 임의의 초기값 설정
W = tf.Variable(2.0)
b = tf.Variable(0.5)

# 값을 0.01큼 반영
learning_rate = 0.01

# tape.gradient를 이용해 미분값(기울기값)을 구함, W에 관한 기울기는 W_grad, b에 관한 기울기는 b_grad로 선언
# for문을 이용해 100번 assign_sub를 이용해 W,b값을 업데이트를 시킴, i가 10의 배수가 될 때마다 print
for i in range(100):
    with tf.GradientTape() as tape:
        hypothesis = W * x_data + b # H(x) = Wx + b (가설 함수)
        cost = tf.reduce_mean(tf.square(hypothesis - y_data)) # cost function 함수 (에러 제곱의 평균값)
    W_grad, b_grad = tape.gradient(cost, [W, b])
    W.assign_sub(learning_rate * W_grad)
    b.assign_sub(learning_rate * b_grad)
    if i % 10 == 0:
      print("{:5}|{:10.4f}|{:10.4f}|{:10.6f}".format(i, W.numpy(), b.numpy(), cost))

print() 

print(W * 5 + b)
print(W * 2.5 + b)
