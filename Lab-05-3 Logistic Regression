import numpy as np # numpy를 np로 import
import matplotlib.pyplot as plt # matplotlib.pyplot를 plt로 import
import tensorflow as tf # tensorflow를 tf로 import
import tensorflow.contrib.eager as tfe # tensorflow.contrib.eager를 tfe로 import

# tensorflow를 enable_eager_execution모드로 실행
tf.enable_eager_execution()
tf.set_random_seed(777)

x_train = [[1., 2.],
          [2., 3.],
          [3., 1.],
          [4., 3.],
          [5., 3.],
          [6., 2.]]
y_train = [[0.],
          [0.],
          [0.],
          [1.],
          [1.],
          [1.]]

x_test = [[5.,2.]]
y_test = [[1.]]

# 가져올 data. tf data를 통해 x_train, y_train값을 가져옴
dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))

# W는 2by1 모양
W = tf.Variable(tf.zeros([2,1]), name='weight')
b = tf.Variable(tf.zeros([1]), name='bias')

# logistic_regression은 원하는 Logistic Regression에 대한 가설
def logistic_regression(features):
    hypothesis  = tf.div(1., 1. + tf.exp(tf.matmul(features, W) + b)) 
    return hypothesis

# 코드 오류를 없애고자 가설 선언, labels와 hypothesis를 통해 원하는 cost값을 구함
def loss_fn(hypothesis, features, labels):
    cost = -tf.reduce_mean(labels * tf.log(logistic_regression(features)) + (1 - labels) * tf.log(1 - hypothesis))
    return cost
    
# 학습을 위한 함수, 실제 값과 가설을 통해 나온 결과값을 비교하는 loss값 구할 수 있음
def grad(hypothesis, features, labels):
    with tf.GradientTape() as tape:
        loss_value = loss_fn(logistic_regression(features),features,labels)
    return tape.gradient(loss_value, [W,b])

# tf.train.GradientDescentOptimizer를 통해 learning_rate=0.01로 optimizer(최적화) 선언
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)

# for문을 이용해 1001만큼 반복하며 업데이트, cost값을 줄여가며 optimizer선언
EPOCHS = 1001
for step in range(EPOCHS):
    for features, labels  in tfe.Iterator(dataset):
        grads = grad(logistic_regression(features), features, labels)
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))
        if step % 100 == 0:  # 100배수마다 print
            print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features),features,labels)))

# accuracy_fn은 가설을 이용해 도출된 값(predicted)과 실제값(labels)을 비교하기 위한 함수
def accuracy_fn(hypothesis, labels):
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))
    return accuracy

# 출력
test_acc = accuracy_fn(logistic_regression(x_test),y_test)
